---
phase: 03-core-intelligence-mode-switching
plan: 04
type: execute
wave: 3
depends_on: ["03-03"]
files_modified: []
autonomous: false
requirements: [CHAT-01, CHAT-02, CHAT-03, CHAT-04, CHAT-05, ARCH-02]

must_haves:
  truths:
    - "All automated tests pass (test_mode_detection.py and test_session_store.py)"
    - "Sending a normal message via the HTTP chat endpoint returns an AI response (not echo)"
    - "Mode switch via /intimate returns the confirmation message"
    - "Mode switch via /stop returns the secretary confirmation message"
    - "After a mode switch, the next message produces a response consistent with the new mode's system prompt"
    - "A user with no avatar receives the onboarding prompt"
  artifacts:
    - path: "backend/app/services/llm/base.py"
      provides: "LLMProvider Protocol"
    - path: "backend/app/services/llm/openai_provider.py"
      provides: "OpenAI concrete provider"
    - path: "backend/app/services/session/store.py"
      provides: "SessionStore singleton"
    - path: "backend/app/services/mode_detection/detector.py"
      provides: "detect_mode_switch()"
    - path: "backend/app/services/chat.py"
      provides: "ChatService.handle_message()"
    - path: "backend/app/routers/webhook.py"
      provides: "Upgraded webhook with AI pipeline"
  key_links:
    - from: "backend/app/routers/webhook.py"
      to: "backend/app/services/chat.py"
      via: "await _chat_service.handle_message()"
      pattern: "handle_message"
    - from: "backend/app/services/chat.py"
      to: "backend/app/services/llm/openai_provider.py"
      via: "await self._llm.complete()"
      pattern: "\\.complete\\("
---

<objective>
Verify the complete Phase 3 pipeline end-to-end: automated test suite passes, the server starts cleanly, and manual testing confirms AI responses, mode switching, and correct session isolation.

Purpose: This checkpoint ensures all five requirements (CHAT-01 through CHAT-05 and ARCH-02) are observable and working before declaring Phase 3 complete.
Output: Verified working system. No new files â€” verification only.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-core-intelligence-mode-switching/03-CONTEXT.md
@.planning/phases/03-core-intelligence-mode-switching/03-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run automated test suite and verify server starts</name>
  <files></files>
  <action>
Run the full test suite:
```bash
cd backend && python -m pytest tests/ -v 2>&1
```

Expected: all tests in test_mode_detection.py and test_session_store.py pass.

Verify server starts without import errors (dry-run import check):
```bash
cd backend && python -c "from app.main import app; print('Server import OK')" 2>&1
```

If import fails due to missing OPENAI_API_KEY, confirm config.py has a default empty string (`openai_api_key: str = ""`). The server should start without the key set (requests will fail at LLM call time, which is expected when the key is not configured).

Verify all critical imports resolve:
```bash
cd backend && python -c "
from app.services.llm.base import LLMProvider, Message
from app.services.llm.openai_provider import OpenAIProvider
from app.services.llm.prompts import secretary_prompt, intimate_prompt
from app.services.session.models import ConversationMode
from app.services.session.store import SessionStore, get_session_store
from app.services.mode_detection.detector import detect_mode_switch, DetectionResult
from app.services.chat import ChatService
from app.services.user_lookup import get_avatar_for_user
print('All Phase 3 imports OK')
" 2>&1
```
  </action>
  <verify>
    <automated>cd backend && python -m pytest tests/ -v 2>&1 | tail -15</automated>
    <manual>Confirm all tests show PASSED (not SKIPPED or FAILED). Confirm import check prints "All Phase 3 imports OK" or shows only configuration warnings (not ImportError).</manual>
  </verify>
  <done>All automated tests pass. All Phase 3 module imports resolve without ImportError.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Human verify Phase 3 AI pipeline end-to-end</name>
  <action>No automated action â€” human verifies the complete pipeline per instructions below.</action>
  <what-built>
Complete Phase 3 AI pipeline:
- LLM service abstraction (LLMProvider Protocol + OpenAIProvider)
- Session store with per-user per-mode history isolation
- Mode switch detector (slash commands + fuzzy phrase matching)
- ChatService orchestrator
- WhatsApp webhook upgraded from echo to AI responses
  </what-built>
  <how-to-verify>
**Prerequisites:** OPENAI_API_KEY must be set in backend/.env before testing. If you don't have one, you can test the mode switching and session logic independently (see Option B below).

**Option A: Full end-to-end with real LLM (requires OPENAI_API_KEY)**

1. Start the server:
   ```
   cd backend && uvicorn app.main:app --reload --port 8000
   ```

2. Create a test user and get a JWT token (use the auth endpoint from Phase 2):
   ```
   curl -X POST http://localhost:8000/auth/signup \
     -H "Content-Type: application/json" \
     -d '{"email":"test@example.com","password":"test1234"}'
   ```

3. Simulate a WhatsApp webhook message (replace USER_ID with the user_id from step 2):
   ```
   curl -X POST http://localhost:8000/webhook \
     -H "Content-Type: application/json" \
     -d '{
       "entry": [{
         "changes": [{
           "value": {
             "messages": [{"from":"+1234567890","type":"text","text":{"body":"Hello, what can you do?"}}],
             "metadata": {"phone_number_id": "test-phone-id"}
           }
         }]
       }]
     }'
   ```
   Expected: No echo "[Echo]" text. Response is {"status":"ok"} (reply sent via WhatsApp if credentials configured).

4. Test mode switch:
   - Send text body: `/intimate`
   - Expected: logs show "Switching to private mode" reply generated

5. Test mode switch back:
   - Send text body: `/stop`
   - Expected: logs show "Back to work mode." reply generated

**Option B: Unit-level verification (no API key needed)**

Run the existing tests and manually verify ChatService logic:
```
cd backend && python -m pytest tests/ -v
```

Verify mode switch confirmation messages are present in chat.py:
```
grep -n "just us now" backend/app/services/chat.py
grep -n "Back to work" backend/app/services/chat.py
grep -n "ONBOARDING_PROMPT" backend/app/services/chat.py
```

Verify echo is gone from webhook:
```
grep -n "Echo" backend/app/routers/webhook.py
```
Expected: no matches.

**Type "approved" when satisfied, or describe any issues found.**
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
**Automated:**
- `cd backend && python -m pytest tests/ -v` â€” all pass
- `grep "Echo" backend/app/routers/webhook.py` â€” no results
- `grep "handle_message" backend/app/routers/webhook.py` â€” found
- `grep "openai_api_key" backend/app/config.py` â€” found

**Manual (with LLM key):**
- Normal message â†’ AI response (not echo)
- `/intimate` â†’ "Switching to private mode â€” just us now ðŸ’¬"
- `/stop` â†’ "Back to work mode."
- Fuzzy "im alone" â†’ same as /intimate (fuzzy match)
- No avatar â†’ onboarding prompt
</verification>

<success_criteria>
Phase 3 is complete when:
1. All automated tests pass (`pytest tests/`)
2. "[Echo]" is absent from webhook.py
3. ChatService.handle_message() handles all 5 cases (no avatar, exact switch, fuzzy switch, ambiguous, normal LLM)
4. Mode switch confirmation messages match CONTEXT.md decisions exactly
5. LLMProvider is a Protocol â€” ARCH-02 swappability confirmed (OpenAIProvider not imported at call sites, only at composition root)
6. Human verifier has typed "approved"
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-intelligence-mode-switching/03-04-SUMMARY.md`
</output>

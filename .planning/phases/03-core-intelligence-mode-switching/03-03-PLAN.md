---
phase: 03-core-intelligence-mode-switching
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - backend/app/services/user_lookup.py
  - backend/app/services/chat.py
  - backend/app/routers/webhook.py
autonomous: true
requirements: [CHAT-01, CHAT-02, CHAT-03, CHAT-04, CHAT-05]

must_haves:
  truths:
    - "Incoming WhatsApp text messages produce an AI response (not echo) using the user's avatar name and personality"
    - "Bot response uses secretary system prompt when session mode is SECRETARY"
    - "Bot response uses intimate system prompt when session mode is INTIMATE"
    - "'/intimate' or 'i'm alone' causes bot to reply with confirmation message and switches session to INTIMATE"
    - "'/stop' or 'stop' causes bot to reply with confirmation message and switches session to SECRETARY"
    - "Ambiguous phrasing produces a clarification question, not an immediate switch"
    - "User with no avatar receives onboarding prompt instead of LLM response"
    - "Message history is persisted to Supabase (both user and assistant turns)"
    - "Avatar name and personality are cached in SessionState after first fetch (no DB call per message)"
  artifacts:
    - path: "backend/app/services/user_lookup.py"
      provides: "get_avatar_for_user() function fetching avatar row by user_id via supabase_admin"
      exports: ["get_avatar_for_user"]
    - path: "backend/app/services/chat.py"
      provides: "ChatService.handle_message() â€” orchestrates mode detection, session, LLM call, and response text"
      exports: ["ChatService"]
    - path: "backend/app/routers/webhook.py"
      provides: "process_whatsapp_message() upgraded from echo to AI pipeline"
      contains: "chat_service"
  key_links:
    - from: "backend/app/routers/webhook.py"
      to: "backend/app/services/chat.py"
      via: "await chat_service.handle_message()"
      pattern: "handle_message"
    - from: "backend/app/services/chat.py"
      to: "backend/app/services/llm/openai_provider.py"
      via: "await self._llm.complete(messages, system_prompt)"
      pattern: "\\.complete\\("
    - from: "backend/app/services/chat.py"
      to: "backend/app/services/session/store.py"
      via: "await session_store.get_or_create(user_id)"
      pattern: "get_or_create"
    - from: "backend/app/services/chat.py"
      to: "backend/app/services/mode_detection/detector.py"
      via: "detect_mode_switch(text, session.mode)"
      pattern: "detect_mode_switch"
---

<objective>
Wire together the LLM service, session store, and mode detector into a ChatService orchestrator, then upgrade webhook.py to use it â€” replacing the Phase 2 echo with real AI responses.

Purpose: Plans 01 and 02 built the pieces; this plan assembles them into a working conversation pipeline end-to-end. After this plan, sending a WhatsApp message produces an actual AI reply.
Output: backend/app/services/chat.py (new), extended user_lookup.py, upgraded webhook.py
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-core-intelligence-mode-switching/03-CONTEXT.md
@.planning/phases/03-core-intelligence-mode-switching/03-RESEARCH.md
@.planning/phases/03-core-intelligence-mode-switching/03-01-SUMMARY.md
@.planning/phases/03-core-intelligence-mode-switching/03-02-SUMMARY.md
@backend/app/services/user_lookup.py
@backend/app/routers/webhook.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend user_lookup.py with get_avatar_for_user() and create ChatService</name>
  <files>
    backend/app/services/user_lookup.py
    backend/app/services/chat.py
  </files>
  <action>
**`backend/app/services/user_lookup.py`** â€” add `get_avatar_for_user()` function after the existing `lookup_user_by_phone()`:

```python
async def get_avatar_for_user(user_id: str) -> dict | None:
    """Fetch the avatar row for a given user_id. Returns dict or None if no avatar.

    Uses supabase_admin (service role) â€” webhook context has no user JWT.
    Returns keys: id, user_id, name, age, personality, physical_description.
    """
    try:
        result = (
            supabase_admin
            .from_("avatars")
            .select("id, user_id, name, age, personality, physical_description")
            .eq("user_id", user_id)
            .single()
            .execute()
        )
        return result.data if result.data else None
    except Exception as e:
        logger.error(f"Avatar lookup failed for user {user_id}: {e}")
        return None
```

---

**`backend/app/services/chat.py`** â€” NEW file. ChatService orchestrates all Phase 3 logic:

```python
"""
ChatService â€” orchestrates mode detection, session management, and LLM calls.

Flow for each incoming message:
  1. Fetch session state (get_or_create)
  2. Cache avatar in session if not yet cached
  3. Run ModeSwitchDetector on incoming text
  4. Route based on detection result:
     a. exact/fuzzy â†’ switch mode (or acknowledge already-in-mode)
     b. ambiguous â†’ set pending_switch_to, return clarification question
     c. pending resolution â†’ handle "yes"/"no" confirmation
     d. none â†’ call LLM with current mode history + system prompt
  5. Append user message and assistant reply to session history
  6. Return reply text (caller sends via WhatsApp or HTTP)
"""
import logging
from dataclasses import dataclass, field
from app.services.session.store import SessionStore, SessionState, get_session_store
from app.services.session.models import ConversationMode, Message
from app.services.mode_detection.detector import detect_mode_switch, DetectionResult
from app.services.llm.base import LLMProvider
from app.services.llm.prompts import secretary_prompt, intimate_prompt

logger = logging.getLogger(__name__)

# Mode switch confirmation messages (per CONTEXT.md decisions)
SWITCH_TO_INTIMATE_MSG = "Switching to private mode â€” just us now ðŸ’¬"
SWITCH_TO_SECRETARY_MSG = "Back to work mode."
ALREADY_INTIMATE_MSG = "We're already in private mode ðŸ˜‰"
ALREADY_SECRETARY_MSG = "We're already in work mode."
CLARIFICATION_TO_INTIMATE_MSG = (
    "Did you mean to switch to private mode? Reply 'yes' or use /intimate."
)
CLARIFICATION_TO_SECRETARY_MSG = (
    "Did you mean to switch back to work mode? Reply 'yes' or use /stop."
)
ONBOARDING_PROMPT = (
    "You haven't set up your Ava profile yet â€” visit ava.example.com to get started."
)
LLM_ERROR_MSG = "I'm having trouble thinking right now â€” try again in a moment."


class ChatService:
    """
    Stateless orchestrator â€” all state lives in SessionStore.

    Designed to be instantiated once as a module-level singleton and called
    from webhook.py and future chat.py router. Thread-safe via SessionStore's asyncio.Lock.
    """

    def __init__(self, llm: LLMProvider, session_store: SessionStore | None = None):
        self._llm = llm
        self._store = session_store or get_session_store()

    async def handle_message(
        self,
        user_id: str,
        incoming_text: str,
        avatar: dict | None,
    ) -> str:
        """
        Process one incoming message and return the reply text.

        Args:
            user_id: Authenticated user's UUID.
            incoming_text: Raw text from WhatsApp or HTTP.
            avatar: Avatar row dict (name, personality) or None if not set up.

        Returns:
            Reply text to send back to the user.
        """
        # Guard: user has no avatar â€” send onboarding prompt
        if avatar is None:
            return ONBOARDING_PROMPT

        session = await self._store.get_or_create(user_id)

        # Cache avatar in session state on first message (avoid DB call per message)
        if not hasattr(session, "_avatar_cache") or session._avatar_cache is None:
            object.__setattr__(session, "_avatar_cache", avatar)

        avatar_name = avatar.get("name", "Ava")
        personality = avatar.get("personality", "caring")

        # --- Clarification gate resolution ---
        if session.pending_switch_to is not None:
            stripped = incoming_text.strip().lower()
            if stripped in ("yes", "y", "yeah", "yep"):
                new_mode = session.pending_switch_to
                await self._store.switch_mode(user_id, new_mode)
                if new_mode == ConversationMode.INTIMATE:
                    return SWITCH_TO_INTIMATE_MSG
                else:
                    return SWITCH_TO_SECRETARY_MSG
            else:
                # User ignored clarification â€” cancel pending switch, route normally
                session.pending_switch_to = None

        # --- Mode switch detection ---
        detection = detect_mode_switch(incoming_text, session.mode)

        if detection.confidence in ("exact", "fuzzy"):
            target = detection.target
            if target == session.mode:
                # Already in this mode â€” acknowledge playfully
                if session.mode == ConversationMode.INTIMATE:
                    return ALREADY_INTIMATE_MSG
                else:
                    return ALREADY_SECRETARY_MSG
            # Switch mode
            await self._store.switch_mode(user_id, target)
            if target == ConversationMode.INTIMATE:
                return SWITCH_TO_INTIMATE_MSG
            else:
                return SWITCH_TO_SECRETARY_MSG

        if detection.confidence == "ambiguous":
            session.pending_switch_to = detection.target
            if detection.target == ConversationMode.INTIMATE:
                return CLARIFICATION_TO_INTIMATE_MSG
            else:
                return CLARIFICATION_TO_SECRETARY_MSG

        # --- Normal message: call LLM ---
        current_mode = session.mode
        history = list(session.history[current_mode])  # snapshot before append

        if current_mode == ConversationMode.SECRETARY:
            system_prompt = secretary_prompt(avatar_name, personality)
        else:
            system_prompt = intimate_prompt(avatar_name, personality)

        user_message: Message = {"role": "user", "content": incoming_text}

        try:
            reply = await self._llm.complete(history + [user_message], system_prompt)
        except Exception as e:
            logger.error(f"LLM call failed for user {user_id}: {e}")
            reply = LLM_ERROR_MSG

        # Append both turns to session history (mode-isolated)
        await self._store.append_message(user_id, current_mode, user_message)
        await self._store.append_message(
            user_id, current_mode, {"role": "assistant", "content": reply}
        )

        return reply
```

Note: `object.__setattr__` is used to set `_avatar_cache` on the dataclass because dataclasses are frozen by default in some configurations. If SessionState is not frozen, add `avatar_cache: dict | None = None` to the SessionState dataclass instead and use `state.avatar_cache = avatar`. Using the dataclass field is cleaner â€” update SessionState in store.py to add `avatar_cache: dict | None = None` as a field.

**Preferred approach:** Update `backend/app/services/session/store.py` SessionState dataclass to add:
```python
avatar_cache: dict | None = None  # cached per session after first DB fetch
```
Then in chat.py use `if session.avatar_cache is None: session.avatar_cache = avatar`.
  </action>
  <verify>
    <automated>python -m py_compile backend/app/services/user_lookup.py && python -m py_compile backend/app/services/chat.py && echo "Syntax OK"</automated>
    <manual>Confirm get_avatar_for_user() is in user_lookup.py. Confirm ChatService.handle_message() covers all branches: no avatar, exact switch, fuzzy switch, ambiguous, normal LLM call. Confirm onboarding message is returned when avatar is None.</manual>
  </verify>
  <done>user_lookup.py has get_avatar_for_user() using supabase_admin. backend/app/services/chat.py exists with ChatService class. handle_message() covers all 5 flow branches. Both files pass py_compile.</done>
</task>

<task type="auto">
  <name>Task 2: Upgrade webhook.py to use ChatService (replace echo with AI pipeline)</name>
  <files>
    backend/app/routers/webhook.py
  </files>
  <action>
Replace the echo handler in `backend/app/routers/webhook.py` with the AI pipeline.

**New webhook.py** â€” keep all existing GET verification handler and outer try/except structure intact, only replace `process_whatsapp_message`:

```python
from fastapi import APIRouter, Request, HTTPException, Query
from app.config import settings
from app.services.whatsapp import send_whatsapp_message
from app.services.user_lookup import lookup_user_by_phone, get_avatar_for_user
from app.services.session.store import get_session_store
from app.services.llm.openai_provider import OpenAIProvider
from app.services.chat import ChatService
from app.database import supabase_admin
import logging

router = APIRouter(prefix="/webhook", tags=["webhook"])
logger = logging.getLogger(__name__)

# Module-level singletons â€” instantiated once per process at import time
# OpenAIProvider uses AsyncOpenAI (non-blocking); safe to share across requests
_llm_provider = OpenAIProvider(
    api_key=settings.openai_api_key,
    model=settings.llm_model,
)
_chat_service = ChatService(llm=_llm_provider, session_store=get_session_store())


@router.get("")
async def verify_webhook(
    hub_mode: str = Query(None, alias="hub.mode"),
    hub_challenge: str = Query(None, alias="hub.challenge"),
    hub_verify_token: str = Query(None, alias="hub.verify_token"),
):
    """
    Meta calls this GET endpoint when you register the webhook URL.
    Returns the challenge value to confirm ownership.
    """
    if hub_mode == "subscribe" and hub_verify_token == settings.whatsapp_verify_token:
        return int(hub_challenge)
    raise HTTPException(status_code=403, detail="Forbidden")


@router.post("")
async def handle_incoming(request: Request):
    """
    Meta delivers incoming WhatsApp messages here.

    Always returns HTTP 200 â€” non-200 causes Meta to retry delivery,
    resulting in duplicate messages. Errors are logged internally.
    """
    try:
        body = await request.json()
        await process_whatsapp_message(body)
    except Exception as e:
        logger.error(f"Webhook processing error: {e}")
        # Still return 200 â€” Meta doesn't need to know about internal errors

    return {"status": "ok"}


async def process_whatsapp_message(body: dict) -> None:
    """Process incoming WhatsApp webhook payload through the AI pipeline."""
    value = body["entry"][0]["changes"][0]["value"]

    if "messages" not in value:
        return  # Delivery receipt or status update â€” ignore

    message = value["messages"][0]
    sender_phone = message["from"]  # E.164 format: +1234567890
    message_type = message.get("type")

    if message_type != "text":
        return  # Only handle text messages in Phase 3

    incoming_text = message["text"]["body"]
    phone_number_id = value["metadata"]["phone_number_id"]

    # Look up user by phone number (service role client â€” bypasses RLS)
    user = await lookup_user_by_phone(sender_phone)

    if user is None:
        # Unlinked number â€” send registration instructions
        await send_whatsapp_message(
            phone_number_id=phone_number_id,
            to=sender_phone,
            text="Please create an account at ava.example.com and link your number",
        )
        return

    user_id = user["user_id"]

    # Fetch avatar (cached in session after first call â€” see ChatService)
    avatar = await get_avatar_for_user(user_id)

    # Run through AI pipeline â€” returns reply text
    reply_text = await _chat_service.handle_message(
        user_id=user_id,
        incoming_text=incoming_text,
        avatar=avatar,
    )

    # Send reply via WhatsApp
    await send_whatsapp_message(
        phone_number_id=phone_number_id,
        to=sender_phone,
        text=reply_text,
    )

    # Log both messages to Supabase (DB failure must not prevent reply â€” already sent above)
    try:
        supabase_admin.from_("messages").insert([
            {
                "user_id": user_id,
                "avatar_id": avatar["id"] if avatar else None,
                "channel": "whatsapp",
                "role": "user",
                "content": incoming_text,
            },
            {
                "user_id": user_id,
                "avatar_id": avatar["id"] if avatar else None,
                "channel": "whatsapp",
                "role": "assistant",
                "content": reply_text,
            },
        ]).execute()
    except Exception as e:
        logger.error(f"Message logging failed for user {user_id}: {e}")
        # DB failure does not prevent the reply â€” already sent above
```

Key changes from Phase 2:
- Removed: `echo_text = f"[Echo] {incoming_text}"` block
- Added: module-level `_llm_provider` and `_chat_service` singletons
- Added: `get_avatar_for_user()` call before ChatService
- Added: `avatar_id` in Supabase message logging (was None hardcoded before)
- Changed: `reply_text` from echo to `_chat_service.handle_message()` result
  </action>
  <verify>
    <automated>python -m py_compile backend/app/routers/webhook.py && echo "webhook syntax OK"</automated>
    <manual>Confirm "[Echo]" string is no longer present. Confirm _chat_service.handle_message() is awaited. Confirm send_whatsapp_message is called BEFORE Supabase logging (DB failure must not block reply).</manual>
  </verify>
  <done>webhook.py has no echo logic. It awaits _chat_service.handle_message(). send_whatsapp_message() is called before Supabase logging. File passes py_compile.</done>
</task>

</tasks>

<verification>
All three files pass `python -m py_compile`.
`grep -n "Echo" backend/app/routers/webhook.py` returns no results.
`grep -n "handle_message" backend/app/routers/webhook.py` shows the ChatService call.
`grep -n "get_avatar_for_user" backend/app/services/user_lookup.py` confirms the new function exists.
`grep -n "send_whatsapp_message" backend/app/routers/webhook.py` appears BEFORE the Supabase logging block.
</verification>

<success_criteria>
- webhook.py no longer contains any echo logic ("[Echo]" string absent)
- process_whatsapp_message() calls get_avatar_for_user() then _chat_service.handle_message()
- ChatService.handle_message() exists and handles: no avatar, exact switch, fuzzy switch, ambiguous, normal LLM
- get_avatar_for_user() in user_lookup.py queries avatars table via supabase_admin
- Supabase message logging includes avatar_id (not None hardcoded)
- All modified files pass py_compile
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-intelligence-mode-switching/03-03-SUMMARY.md`
</output>
